[
  {
    "objectID": "CHEATSHEET.html",
    "href": "CHEATSHEET.html",
    "title": "Sydney Informatics Hub Course name",
    "section": "",
    "text": "Quarto template cheatsheet\nPlease contribute your tips, tricks for use, customisation of this template :)\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nextflow Tower Service User Guide",
    "section": "",
    "text": "Nextflow Tower is a full-stack solution for the management of Nextflow pipeline executions. It’s designed as an intuitive, user-friendly interface that allows you to control and monitor your data analysis pipelines. It simplifies running complex workflows, like those provided by nf-core, on different computation infrastructures such as HPC clusters, cloud services, or Kubernetes.\nThe Tower service is offered in two ways; a free basic account that can be used by individuals and a licensed service that is suitable for large organisations with multiple collaborators that require more complex user management. The Australian BioCommons Nextflow Tower service is licensed and provides a centralised command-post for running Nextflow pipelines as a subsidised service for Australian researchers.\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html#trainers",
    "href": "index.html#trainers",
    "title": "Course title",
    "section": "Trainers",
    "text": "Trainers\nHave a list of the people delivering the course if it is needed. However, this is subject to change so might be a good idea to not specifiy.\nHere’s an example list. This is a good place to define what your preferred name is if you have one.\n\nNathaniel (Nate) Butterworth\nDarya Vanichkina\nKristian Maras"
  },
  {
    "objectID": "index.html#course-pre-requisites-and-setup-requirements",
    "href": "index.html#course-pre-requisites-and-setup-requirements",
    "title": "Course title",
    "section": "Course pre-requisites and setup requirements",
    "text": "Course pre-requisites and setup requirements\nProvide some sort of general description of what you should know and who this course is for. For example:\n\nNo previous programming experience is required, but Session 1 is a pre-requisite for the other sessions. Training will be delivered online, so you will need access to a modern computer with a stable internet connection and around 5GB of storage space for data downloaded prior to the course. Participants are encouraged to setup a Python environment on their local computer (as per the Setup Instructions provided), but participation using other platforms/environments can be supported where necessary."
  },
  {
    "objectID": "index.html#venue",
    "href": "index.html#venue",
    "title": "Course title",
    "section": "Venue",
    "text": "Venue\nSpecifiy where the training will be, whether it is in person, online, or hybrid.\nInclude a reminder about the location and time zone in which this course will be running. In almost all cases, our courses are delivered from Sydney and times are reported in AEDT (GMT+11).\n\nZoom etiquette and how we interact\nInclude some sort of descripter about what is expected from attendees in terms of participation, and what sort of interaction they should expect. This could also be moved to an about page.\nHere’s an example for a zoom only online course:\n\nSessions will be recorded for attendees only, and it is set up to only record the host shared screen and host audio. We will try and get these uploaded to this site as soon as possible. Please interrupt whenever you want! Ideally, have your camera on and interact as much as possible. There will be someone monitoring the chat-window with any questions you would like to post there. Four hours is a long Zoom session so we have plenty of scheduled breaks combined with a mix of content to be delivered as demos, plus sections as independent exercises, but most of the course will be pretty-hands on with everyone writing their own code. We will use Zoom break-out rooms as needed with the Trainers and participants."
  },
  {
    "objectID": "index.html#code-of-conduct",
    "href": "index.html#code-of-conduct",
    "title": "Course title",
    "section": "Code of Conduct",
    "text": "Code of Conduct\nAs a University of Sydney course, we also want to make sure peopele are aware of our code of conduct. Feel free to move this to an about page as needed.\nExample standard Code of Conduct statement:\nWe expect all attendees of our training to follow our code of conduct, including bullying, harassment and discrimination prevention policies.\nIn order to foster a positive and professional learning environment we encourage the following kinds of behaviours at all our events and on our platforms:\n\nUse welcoming and inclusive language\nBe respectful of different viewpoints and experiences\nGracefully accept constructive criticism\nFocus on what is best for the community\nShow courtesy and respect towards other community members\n\nOur full CoC, with incident reporting guidelines, is available [here]https://sydney-informatics-hub.github.io/codeofconduct/)."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Course title",
    "section": "Schedule",
    "text": "Schedule\nIf your training includes multiple components, it is always nice to have a table with the schedule. With quarto we can make tables using markdown formating, like below.\n\n\n\nDay 1\n\n\n\nMorning\nFundamentals\n\n\n\nExample R Content\n\n\nDay 2\n\n\n\nMorning\nExample MD Content"
  },
  {
    "objectID": "index.html#setup-instructions",
    "href": "index.html#setup-instructions",
    "title": "Course title",
    "section": "Setup Instructions",
    "text": "Setup Instructions\nLink to the setup instructions if there are any, for example:\n\nPlease complete the Setup Instructions before the course. If you have any trouble, please get in contact with us ASAP."
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "User Setup Instructions",
    "section": "",
    "text": "All materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01a-fundamentals.html",
    "href": "notebooks/01a-fundamentals.html",
    "title": "Sydney Informatics Hub Course name",
    "section": "",
    "text": "Python fundamentals\n\nQuestions\n\nWhat can Python do?\nHow do I do it?\n\n\n\nObjectives\n\nLearn the basic Python commands\n\n\nGenerally, cells like this are what to type into your Python shell/notebook/colab:\n\n2+4*10\n\n42\n\n\n\nFunctions\nThese are bits of code you want to perhaps use many times, or keep self contained, or refer to at different points. They can take values as input and give values back (or not).\n\n#Declare the name of the function\ndef add_numbers(x,y):\n    '''adds two numbers\n    usage: myaddition=addnumbers(x,y)\n    returns: z\n    inputs: x,y\n    x and y are two integers\n    z is the summation of x and y\n    '''\n    \n    z=x+y\n    \n    return(z)\n\nNote the indentation - Python forces your code to be nicely readable by using ‘whitespace’/indentation to signify what chunks of code are related. You will see this more later, but generally you should try and write readable code and follow style standards\nMany functions have a header - formatted as a multiline comment with three ’’’. This hopefully will tell you about the function\nAnyway, let’s run our function, now that we have initialised it!\n\nadd_numbers(1,2)\n\n3\n\n\n\n\nChallenge\nWrite a function to convert map scale. For example, on a 1:25,000 map (good for hiking!) the distance between two points is 15 cm. How far apart are these in real life? (3750 m).\n[Reminder: 15 cm * 25000 = 375000 cm = 3750 m]\nYour function should take as input two numbers: the distance on the map (in cm) and the second number of the scale and, i.e. calculate_distance(15, 25000) should return 375000\n\n\nSolution\n\n#Declare the name of the function\ndef calculate_distance(distance_cm,scale):\n    '''calculates distance based on map and scale\n    returns: z\n    inputs: distance_cm,scale\n    distance_cm and scale are two integers\n    returns the product of distance_cm and scale\n    '''  \n    \n    return(distance_cm * scale)\n\n\n\n#First we have to load some modules to do the work for us.\n#Modules are packages people have written so we do not have to re-invent everything!\n\n#The first is NUMerical PYthon. A very popular matrix, math, array and data manipulation library.\nimport numpy as np\n\n#This is a library for making figures (originally based off Matlab plotting routines)\n#We use the alias 'plt' because we don't want to type out the whole name every time we reference it!\nimport matplotlib.pyplot as plt \n\n# random code from matplotlib docs\n# Fixing random state for reproducibility\nnp.random.seed(19680801)\n\n\nplt.rcdefaults()\nfig, ax = plt.subplots()\n\n# Example data\npeople = ('Tom', 'Dick', 'Harry', 'Slim', 'Jim')\ny_pos = np.arange(len(people))\nperformance = 3 + 10 * np.random.rand(len(people))\nerror = np.random.rand(len(people))\n\nax.barh(y_pos, performance, xerr=error, align='center')\nax.set_yticks(y_pos, labels=people)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Performance')\nax.set_title('How fast do you want to go today?')\n\nplt.show()\n\n\n\n\n\nKey points\n\nYou can store things in Python in variables\nLists can be used to store objects of different types\nLoops with for can be used to iterate over each object in a list\nFunctions are used to write (and debug) repetitive code once\nIndexing\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01c_exampleMDcontent.html",
    "href": "notebooks/01c_exampleMDcontent.html",
    "title": "Sydney Informatics Hub Course name",
    "section": "",
    "text": "Sample lesson from markdown\n\nQuestions\n\nWhat can I do?\nHow do I do it?\n\n\n\nObjectives\n\nLearn the basic Unix commands\n\n\n# I am a comment\npwd\n\nfor i in $(seq 1 10); do echo $i;done\n\nChallenge\nThis is a challenge question\n\n\nSolution\n\n\n#Declare the name of the function\necho \"Joy\"\n\n\n\nKey points\n\nKey\nPoints\nHere\n\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "notebooks/01b_exampleRcontent.html",
    "href": "notebooks/01b_exampleRcontent.html",
    "title": "Sydney Informatics Hub Course name",
    "section": "",
    "text": "Sample R lesson\n\n\n\n\n\n\nChallenge 1\n\n\n\nHow can we make our lessons a bit more interactive??\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nHere’s some markdown code showing how to use markdown to do cool things in our quarto markdowns.\n\n::: callout-tip\n## Challenge 1\nHow can we make our lessons a bit more interactive??\n:::\n\n::: {.callout-caution collapse=\"true\"}\n## Solution\n\nWe can take advantage of cool markdown!\n\n:::\n\n\n\nYou can define sections in your quarto using html.\n<div class=\"questions\">\n</div>\nQuestions\n\nWhat can R do?\nHow do I do it?\nObjectives\n\nLearn the basic R commands\n\n# if using VSCode may been\n#install.packages(\"languageserver\")\nhist(mtcars$mpg)\n\n\n\n\nChallenge\nWrite a function to convert map scale. For example, on a 1:25,000 map (good for hiking!) the distance between two points is 15 cm. How far apart are these in real life? (3750 m).\n[Reminder: 15 cm * 25000 = 375000 cm = 3750 m]\nYour function should take as input two numbers: the distance on the map (in cm) and the second number of the scale and, i.e. calculate_distance(15, 25000) should return 375000\n\nSolution\n#Declare the name of the function\n# this is not a R example - but you get the idea!\ndef calculate_distance(distance_cm,scale):\n    '''calculates distance based on map and scale\n    returns: z\n    inputs: distance_cm,scale\n    distance_cm and scale are two integers\n    returns the product of distance_cm and scale\n    '''  \n    \n    return(distance_cm * scale)\nKey points\n\nYou can store things in R in variables\nLists can be used to store objects of different types\nLoops with for can be used to iterate over each object in a list\nFunctions are used to write (and debug) repetitive code once\nIndexing\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Nextflow Tower Service User Guide",
    "section": "Resources",
    "text": "Resources\n\nBiocommons tower service\nTower-nf configuration scripts"
  },
  {
    "objectID": "index.html#getting-started-with-the-australian-biocommons-service",
    "href": "index.html#getting-started-with-the-australian-biocommons-service",
    "title": "Nextflow Tower Service User Guide",
    "section": "Getting started with the Australian BioCommons service",
    "text": "Getting started with the Australian BioCommons service\nNextflow Tower provides a centralised interface for managing users, workspaces, compute environments, and workflows within defined spaces called workspaces. These workspaces are provided by the Australian BioCommons to use cases for the pilot program of the Australian BioCommons Nextflow Tower service. Therefore,  the Australian BioCommons service team has insight into and administrative control over workspaces. However, workspace management and curation is the responsibility of the workspace administrator(s) from the research group. Only organisation owners are permitted to create a workspace within the organisation. More information on workspace management can be found here: workspace management.\n\n\n\nA workspace provides the context in which a user operates, including what resources are available and who can access them. It is composed of pipelines, compute environments, credentials, runs, actions, and datasets. Access permissions are controlled through participants, collaborators, and teams.\n\n\n\n\nOrganizations: An organization is the top-level entity where businesses, institutions, and groups can collaborate. It can contain multiple workspaces.\nOrganization Owner: TBA\nTeam: A team is a group of members in the same organization. Teams can operate in one or more organization workspaces with a specific workspace role (one role per workspace).\nParticipant: A user operating with an assigned role within a workspace.\nParticipant Role: The participant role defines the permissions granted to a user to perform actions or tasks within a workspace.\n\n\nAs you use Tower, keep in mind there are a number of key terms and core concepts that you will come across as you set up your workspace, configure your compute environments, and execute your workflows."
  },
  {
    "objectID": "index.html#log-into-the-service",
    "href": "index.html#log-into-the-service",
    "title": "Nextflow Tower Service User Guide",
    "section": "Log into the service",
    "text": "Log into the service\nNextflow Tower provides a centralised interface for managing users, workspaces, compute environments, and workflows within defined spaces called workspaces. These workspaces are provided by the Australian BioCommons and workspace management and curation is relegated to  the workspace administrators. Regarding workspace management, this means that:\n\n\n\nGo to https://tower.services.biocommons.org.au.\nProvide a your University email address, then you will receive an email with the access link in your email inbox.\nClick the workspace dropdown menu at the top left of the page with your name to navigate between your personal and organisation workspaces.\nNavigate from your personal workspace to your group's workspace by selecting the down arrow next to your name on the top left of the interface.\n\n\n\n\n\n#replace this image with a higher resolution one"
  },
  {
    "objectID": "index.html#the-interface",
    "href": "index.html#the-interface",
    "title": "Nextflow Tower Service User Guide",
    "section": "The interface",
    "text": "The interface\nUpon signing into Tower, you will be greeted with an interface for you as an individual user. To access the full functionality offered by the Australian BioCommons Tower service, you will need to move into (one of) your assigned BioCommons workspaces. To do this:\n\n\nSelect the dropdown menu by your user name.\nThen select the BioCommons workspace of interest.\n\n\nThis is the dashboard where you will manage your workspaces and workflows. It will open to the launchpad interface. Above the interface you'll see the following:\n\n#replace this image with a higher resolution one\n\n\n\nLaunchpad: all your configured pipelines will be available here for running. Run them from here by selecting the 'Launch' button for the pipeline of interest. \nRuns: from here you can monitor and inspect the details of all previous workflow executions that you and other users in your workspace have run.\nActions: you can use Github and Tower webhooks to trigger an automated pipeline test run based on things like pipeline version release. \nDatasets: you can store pipeline input sample sheets here that specify metadata and file paths for each of your datapoints or samples to be processed by a workflow. \nCompute environment: Tower can be used to deploy workflows on various environments, here you will be able to manage and find your preconfigured environments. \nCredentials: here you can store all your authentication keys, credentials required to access private code repositories on GitHub, the Tower agent on HPCs, and external services. These are all encrypted, so they cannot be accessed by anyone else once you've stored them. \nSecrets: Ignore for now. \nParticipants: here you can see and manage (if you are an admin) the other users in your workspace and define user roles to control access. \nSettings: manage some basic workspace settings (if you are an admin)."
  },
  {
    "objectID": "index.html#set-up-your-workspace",
    "href": "index.html#set-up-your-workspace",
    "title": "Nextflow Tower Service User Guide",
    "section": "Set up your workspace",
    "text": "Set up your workspace\nWorkspace configuration can only be performed by admins. You can create and configure compute environments, configure workflows to run on these environments, and add collaborators. The NeuroCommunity are currently only trialling the Tower service at NCI Gadi and Pawsey Setonix HPCs. As such, you will need to create Tower Agent credentials as this is the only way to access NCI and Pawsey HPC's with Tower."
  },
  {
    "objectID": "setup.html#the-interface",
    "href": "setup.html#the-interface",
    "title": "User Setup Instructions",
    "section": "The interface",
    "text": "The interface\nUpon signing into Tower, you will be greeted with an interface for you as an individual user. To access the full functionality offered by the Australian BioCommons Tower service, you will need to move into (one of) your assigned BioCommons workspaces. To do this:\n\n\nSelect the dropdown menu by your user name.\nThen select the BioCommons workspace of interest.\n\n\nThis is the dashboard where you will manage your workspaces and workflows. It will open to the launchpad interface. Above the interface you’ll see the following:\n\n\n\n\nLaunchpad: all your configured pipelines will be available here for running. Run them from here by selecting the ‘Launch’ button for the pipeline of interest. \nRuns: from here you can monitor and inspect the details of all previous workflow executions that you and other users in your workspace have run.\nActions: you can use Github and Tower webhooks to trigger an automated pipeline test run based on things like pipeline version release. \nDatasets: you can store pipeline input sample sheets here that specify metadata and file paths for each of your datapoints or samples to be processed by a workflow. \nCompute environment: Tower can be used to deploy workflows on various environments, here you will be able to manage and find your preconfigured environments. \nCredentials: here you can store all your authentication keys, credentials required to access private code repositories on GitHub, the Tower agent on HPCs, and external services. These are all encrypted, so they cannot be accessed by anyone else once you’ve stored them. \nSecrets: Ignore for now. \nParticipants: here you can see and manage (if you are an admin) the other users in your workspace and define user roles to control access. \nSettings: manage some basic workspace settings (if you are an admin)."
  },
  {
    "objectID": "setup.html#set-up-your-workspace",
    "href": "setup.html#set-up-your-workspace",
    "title": "User Setup Instructions",
    "section": "Set up your workspace",
    "text": "Set up your workspace\nWorkspace configuration can only be performed by admins. With workspace configuration, you can create and configure compute environments, configure workflows to run on these environments, and add collaborators. The NeuroCommunity are currently only trialling the Tower service at NCI Gadi and Pawsey Setonix HPCs. As such, you will need to create Tower Agent credentials because this is the only way to access NCI and Pawsey HPC’s with Tower.\n\nCreate your personal access token\nYou will need to create a personal access token and store it on your compute environment (i.e. on the HPC environment, not locally) in order to communicate with the Tower API. To create a personal access token you will need to navigate to the user settings logo in the top navbar and select ‘Your tokens’, then:\n\n\nSelect ‘Add Token’. \nName your token something descriptive and select ‘Add’.\n\nCopy the token to your clipboard.\n\nLog in to your HPC (either NCI Gadi or Pawsey Setonix) on the command-line.\n\nIf you are using Gadi, navigate to $HOME.\nIf you are on Setonix, navigate to $MYSOFTWARE.\n\nFrom within the directory mentioned above, make a hidden directory called tower to store Tower token and credentials in.\nCopy your personal access token into a file named token within this directory.\n\n\n\n\n\nCreate a Tower credential for your HPC\nYou can create different types of credentials for different purposes through the Tower interface. For example, you can add credentials to access code repositories, commercial cloud, SSH-keys, and the Tower Agent. To connect your Tower agent to your HPC, you will need to create a Tower agent personal credential.\n\n\nSelect ‘Add Credential’.\nName the credential something descriptive.\nSelect Tower Agent from the Provider dropdown menu.\nEnable ‘Shared agent’ so that all workspace users can access the same instance (optional).\nCopy the Agent connection ID to your clipboard.\nBefore saving the credential you will need to run the agent on your HPC command line. \nLog in to your HPC project space on the command-line and navigate to the shared space where you’ll be running the tower agent from. \nCopy your personal access token into a file named connection_id within the hidden .tower directory.\n\n\n\nRun the run_tower_agent_pbs.sh script at /g/data/er01/tower-nf.\nBack in the tower interface, hit ‘Add’ to save your Tower agent credential.\nYou can cancel the Tower agent running on the command line after the credential is saved.\n\n\n\n\nConfigure a compute environment \nCompute environments have been configured for our group on NCI Gadi and Pawsey Setonix HPCs. Currently only NCI Gadi is a shared environment. The NCI Gadi environment has specifically been configured for our SIH project ‘er01’. Only users with access to er01 added to our workspace will be able to run workflows using this configured compute environment. The process for setting up this workspace is as follows: \n\n\nRun the tower agent on the HPC command lineCLI again with the relevant run_tower_agent_<pbs|slurm>.sh script.\nIn the Tower interface, navigate to ‘Compute Environments’ in the top navbar menu.\nName the environment something descriptive to distinguish it from other environments you may set up.\nSelect the platform. For Setonix this is Slurm workload manager, for Gadi this is Altair PBS Pro. \nProvide your (shared or personal) credentials previously saved to the connection_id file.\n\n\nFor those working on Gadi:\n\n\nIt’s important that you leave the head and compute queue names empty, given jobs submitted to queues on Gadi are transferred to the exec version of each queue once running. The exec queue names are not visible to the Tower API so your jobs will fail to run. \nUnder ‘Staging options’ Singularity and Nextflow modules have been preloaded to be executed before the pipeline launches \nUnder ‘Advanced options’ we have set the Nextflow queue size as 1000 (as directed here)\nUnder ‘Advanced options’ we have set the Head job submit #PBS options and applied head jobs to the compute jobs :\n\nWalltime 10 hours \nNCPUS 1 \nMemory 8G \nAccess to /scratch/iz89\nProject iz89 \nQueue: copyq (as this provides external network access)\n\n\n\nFor those working on Setonix:\n\n\nTBA\n\nTBA"
  },
  {
    "objectID": "notebooks/01_gsv.html",
    "href": "notebooks/01_gsv.html",
    "title": "Sydney Informatics Hub",
    "section": "",
    "text": "Add the Germline-StructuralV-nf Pipeline\nhttps://github.com/Sydney-Informatics-Hub/Germline-StructuralV-nf\n\n\nDescription\nGermlineStructuralV-nf is a pipeline for identifying structural variant events in human Illumina short read whole genome sequence data. GermlineStructuralV-nf identifies structural variant and copy number events from BAM files using Manta, Smoove, and TIDDIT. Variants are then merged using SURVIVOR, and annotated by AnnotSV. The pipeline is written in Nextflow and uses Singularity/Docker to run containerised tools.\nStructural and copy number detection is challenging. Most structural variant detection tools infer these events from read mapping patterns, which can often resemble sequencing and read alignment artefacts. To address this, GermlineStructuralV-nf employs 3 general purpose structural variant calling tools, which each support a combination of detection methods. Manta, Smoove and TIDDIT use typical detection approaches that consider:\n\nDiscordant read pair alignments\nSplit reads that span a breakpoints\nRead depth profiling\nLocal de novo assembly\n\nThis approach is currently considered the best approach for maximising sensitivty of short read data (Cameron et al. 2019, Malmoud et al. 2019). By using a combination of tools that employ different methods, we improve our ability to detect different types and sizes of variant events.\n\n\n\n\n\nAll materials copyright Sydney Informatics Hub, University of Sydney"
  },
  {
    "objectID": "index.html#glossary-of-common-terms",
    "href": "index.html#glossary-of-common-terms",
    "title": "Nextflow Tower Service User Guide",
    "section": "Glossary of common terms",
    "text": "Glossary of common terms\n\n\n\n\n\n\n\n\nDefinition\n\n\n\n\nActions\nActions are used to automate the execution of pre-configured workflows (pipelines), based on event triggers such as code commits and webhooks.\n\n\nCache\nA feature that allows Nextflow to store and reuse previously computed results to avoid redundant computations.\n\n\nCluster\nA network of computers or servers used for distributed computing and task execution.\n\n\nCompute Environments\nA compute environment is the platform where workflows are executed. It is composed of access credentials, configuration settings, and storage options for the environment.\n\n\nConfiguration File\nA file containing settings and parameters that control how a Nextflow workflow is executed.\n\n\nCredentials\nCredentials are access keys stored by Tower in an encrypted format, using AES-256 encryption. They allow the safe storage of authentication keys for compute environments, private code repositories, and external services.\n\n\nDatasets\nDatasets are collections of versioned, structured data, usually in TSV (tab-separated values) and CSV (comma-separated values) formats. They are used to manage sample sheets and metadata, to be validated and used as inputs for workflow executions.\n\n\nGitHub/GitLab\nVersion control platforms used to manage and collaborate on Nextflow workflow scripts and associated files.\n\n\nLaunchpad\nThe Launchpad contains the collection of available pipelines that can be run in a workspace. From here, you can view and select pre-configured pipelines for launch.\n\n\nMembers\nA member is a user who is internal to the organization. Members have an organization role and can operate in one or more organization workspaces. In each workspace, members can have a participant role that defines the permissions granted to them within that workspace.\n\n\nPipeline\nA pipeline is a pre-configured workflow that can be used by all users in a workspace. It is composed of a workflow repository, launch parameters, and a compute environment.\n\n\nPipeline Secrets\nPipeline secrets are keys used by workflow tasks to interact with external systems, such as a password to connect to an external database or an API token. They are stored in Tower using AES-256 encryption.\n\n\nRun\nA run is a workflow execution. The Runs view is used to monitor and inspect the details of workflow executions in a workspace.\n\n\nShared Environment\nTBA\n\n\nTask\nA single unit of work within a workflow. It typically represents a specific analysis step, such as aligning sequences, running statistical analyses, or generating plots.\n\n\nWorkflow\nA sequence of tasks or processes arranged in a specific order to achieve a specific goal, often in the context of scientific or data analysis tasks.\n\n\nWorkspace\nA workspace provides the context in which a user operates, including what resources are available and who can access them. It is composed of pipelines, compute environments, credentials, runs, actions, and datasets. Access permissions are controlled through participants, collaborators, and teams."
  },
  {
    "objectID": "index.html#organisational-structure",
    "href": "index.html#organisational-structure",
    "title": "Nextflow Tower Service User Guide",
    "section": "Organisational structure",
    "text": "Organisational structure\n\n\n\n\n\n\n\n\nRole\n\n\nOrganizations\nAn organization is the top-level entity where businesses, institutions, and groups can collaborate. It can contain multiple workspaces.\n\n\nMembers\nA member is a user who is internal to the organization. Members have an organization role and can operate in one or more organization workspaces. In each workspace, members can have a participant role that defines the permissions granted to them within that workspace.\n\n\nTeam\nA team is a group of members in the same organization. Teams can operate in one or more organization workspaces with a specific workspace role (one role per workspace).\n\n\nParticipant\nA user operating with an assigned role within a workspace.\n\n\nParticipant Role\nThe participant role defines the permissions granted to a user to perform actions or tasks within a workspace."
  },
  {
    "objectID": "index.html#organizational-structure",
    "href": "index.html#organizational-structure",
    "title": "Nextflow Tower Service User Guide",
    "section": "Organizational structure",
    "text": "Organizational structure\n\n\n\n\n\n\n\n\nRole\n\n\nOrganizations\nAn organization is the top-level entity where businesses, institutions, and groups can collaborate. It can contain multiple workspaces.\n\n\nOrganization Owner\nTBA\n\n\nMembers\nA member is a user who is internal to the organization. Members have an organization role and can operate in one or more organization workspaces. In each workspace, members can have a participant role that defines the permissions granted to them within that workspace.\n\n\nTeam\nA team is a group of members in the same organization. Teams can operate in one or more organization workspaces with a specific workspace role (one role per workspace).\n\n\nParticipant\nA user operating with an assigned role within a workspace.\n\n\nParticipant Role\nThe participant role defines the permissions granted to a user to perform actions or tasks within a workspace."
  },
  {
    "objectID": "index.html#glossary-of-terms",
    "href": "index.html#glossary-of-terms",
    "title": "Nextflow Tower Service User Guide",
    "section": "Glossary of terms",
    "text": "Glossary of terms\n\n\n\n\n\n\n\n\nDefinition\n\n\n\n\nActions\nActions are used to automate the execution of pre-configured workflows (pipelines), based on event triggers such as code commits and webhooks.\n\n\nCache\nA feature that allows Nextflow to store and reuse previously computed results to avoid redundant computations.\n\n\nCluster\nA network of computers or servers used for distributed computing and task execution.\n\n\nCompute Environments\nA compute environment is the platform where workflows are executed. It is composed of access credentials, configuration settings, and storage options for the environment.\n\n\nConfiguration File\nA file containing settings and parameters that control how a Nextflow workflow is executed.\n\n\nCredentials\nCredentials are access keys stored by Tower in an encrypted format, using AES-256 encryption. They allow the safe storage of authentication keys for compute environments, private code repositories, and external services.\n\n\nDatasets\nDatasets are collections of versioned, structured data, usually in TSV (tab-separated values) and CSV (comma-separated values) formats. They are used to manage sample sheets and metadata, to be validated and used as inputs for workflow executions.\n\n\nGitHub/GitLab\nVersion control platforms used to manage and collaborate on Nextflow workflow scripts and associated files.\n\n\nLaunchpad\nThe Launchpad contains the collection of available pipelines that can be run in a workspace. From here, you can view and select pre-configured pipelines for launch.\n\n\nMembers\nA member is a user who is internal to the organization. Members have an organization role and can operate in one or more organization workspaces. In each workspace, members can have a participant role that defines the permissions granted to them within that workspace.\n\n\nPipeline\nA pipeline is a pre-configured workflow that can be used by all users in a workspace. It is composed of a workflow repository, launch parameters, and a compute environment.\n\n\nPipeline Secrets\nPipeline secrets are keys used by workflow tasks to interact with external systems, such as a password to connect to an external database or an API token. They are stored in Tower using AES-256 encryption.\n\n\nRun\nA run is a workflow execution. The Runs view is used to monitor and inspect the details of workflow executions in a workspace.\n\n\nShared Environment\nCould refer to a cluster, cloud computing environment, or High-Performance Computing (HPC) system where different users or workflows share the available resources for task execution.\n\n\nTask\nA single unit of work within a workflow. It typically represents a specific analysis step, such as aligning sequences, running statistical analyses, or generating plots.\n\n\nWorkflow\nA sequence of tasks or processes arranged in a specific order to achieve a specific goal, often in the context of scientific or data analysis tasks.\n\n\nWorkspace\nA workspace provides the context in which a user operates, including what resources are available and who can access them. It is composed of pipelines, compute environments, credentials, runs, actions, and datasets. Access permissions are controlled through participants, collaborators, and teams."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Nextflow Tower Service User Guide",
    "section": "Getting started",
    "text": "Getting started\nNextflow Tower provides a centralised interface for managing users, compute environments, and workflows within defined spaces called workspaces. These workspaces are provided by the Australian BioCommons to use cases for the pilot program of the Australian BioCommons Nextflow Tower service. Therefore,  the Australian BioCommons service team has insight into and administrative control over workspaces. However, workspace management and curation is the responsibility of the workspace administrator(s) from the research group. Only organisation owners are permitted to create a workspace within the organisation. More information on workspace management can be found here: workspace management.\n\n\n\nA workspace provides the context in which a user operates, including what resources are available and who can access them. It is composed of pipelines, compute environments, credentials, runs, actions, and datasets. Access permissions are controlled through participants, collaborators, and teams.\n\n\n\n\nOrganization: Top-level entity where businesses, institutions, and groups can collaborate. It can contain multiple workspaces.\nTeam: Group of members in the same organization. Teams can operate in one or more organization workspaces with a specific workspace role (one role per workspace).\nParticipant: A user operating with an assigned role within a workspace.\nParticipant Role: The participant role defines the permissions granted to a user to perform actions or tasks within a workspace.\n\n\nAs you use Tower, keep in mind there are a number of key terms and core concepts that you will come across as you set up your workspace, configure your compute environments, and execute your workflows."
  }
]