---
title: "User Setup Instructions"
output:
  html_document:
    toc: false
    toc_float: false
---

![](images/Screen%20Shot%202023-08-24%20at%202.58.13%20pm.png)

#replace this image with a higher resolution one

### Log into the service

Nextflow Tower provides a centralised interface for managing users, workspaces, compute environments, and workflows within defined spaces called workspaces. These workspaces are provided by the Australian BioCommons and workspace management and curation is relegated to  the workspace administrators. Regarding [workspace management](https://help.tower.nf/23.1/orgs-and-teams/workspace-management/), this means that:

<div>

> -   Go to <https://tower.services.biocommons.org.au>.
>
> -   Provide a your University email address, then you will receive an email with the access link in your email inbox.
>
>     ![](images/Screen%20Shot%202023-08-24%20at%203.07.50%20pm.png)
>
> -   Click the workspace dropdown menu at the top left of the page with your name to navigate between your personal and organisation workspaces.
>
> -   Navigate from your personal workspace to your group\'s workspace by selecting the down arrow next to your name on the top left of the interface.
>
>     ![](images/Screen%20Shot%202023-08-24%20at%202.16.26%20pm.png)

</div>

#replace this image with a higher resolution one

## The interface

Upon signing into Tower, you will be greeted with an interface for you as an individual user. To access the full functionality offered by the Australian BioCommons Tower service, you will need to move into (one of) your assigned BioCommons workspaces. To do this:

> -   Select the dropdown menu by your user name.
>
> -   Then select the BioCommons workspace of interest.

This is the dashboard where you will manage your workspaces and workflows. It will open to the launchpad interface. Above the interface you\'ll see the following:

![](images/Screen%20Shot%202023-08-24%20at%202.17.37%20pm.png)

#replace this image with a higher resolution one

<div>

> 1.  [Launchpad](https://help.tower.nf/23.1/launch/launchpad/?h=launchpad): all your configured pipelines will be available here for running. Run them from here by selecting the \'Launch\' button for the pipeline of interest. 
>
> 2.  [Runs](https://help.tower.nf/23.1/monitoring/overview/?h=runs): from here you can monitor and inspect the details of all previous workflow executions that you and other users in your workspace have run.
>
> 3.  [Actions](https://help.tower.nf/23.1/pipeline-actions/overview/?h=actions): you can use Github and Tower webhooks to trigger an automated pipeline test run based on things like pipeline version release. 
>
> 4.  [Datasets](https://help.tower.nf/23.1/datasets/overview/): you can store pipeline input sample sheets here that specify metadata and file paths for each of your datapoints or samples to be processed by a workflow. 
>
> 5.  [Compute environment](https://help.tower.nf/23.1/enterprise/configuration/compute_environments/): Tower can be used to deploy workflows on various environments, here you will be able to manage and find your preconfigured environments. 
>
> 6.  Credentials: here you can store all your authentication keys, credentials required to access private code repositories on [GitHub](https://help.tower.nf/23.1/enterprise/configuration/authentication/?#github-identity-provider), the [Tower agent](https://help.tower.nf/23.1/credentials/agent_credentials/) on HPCs, and [external services](https://help.tower.nf/23.1/enterprise/configuration/authentication/). These are all encrypted, so they cannot be accessed by anyone else once you\'ve stored them. 
>
> 7.  Secrets: Ignore for now. 
>
> 8.  [Participants](https://help.tower.nf/23.1/orgs-and-teams/workspace-management/?h=#participant-roles): here you can see and manage (if you are an admin) the other users in your workspace and define user roles to control access. 
>
> 9.  Settings: manage some basic workspace settings (if you are an admin).

</div>

## Set up your workspace

Workspace configuration can only be performed by admins. You can create and configure compute environments, configure workflows to run on these environments, and add collaborators. The NeuroCommunity are currently only trialling the Tower service at NCI Gadi and Pawsey Setonix HPCs. As such, you will need to create Tower Agent credentials as this is the only way to access NCI and Pawsey HPC\'s with Tower.

### Create your personal access token for NCI Gadi

You will need to create a personal access token and store it on your compute environment in order to communicate with the Tower API. To create a personal access token you will need to navigate to the user settings logo in the top navbar and select \'Your tokens\', then:

> -   Select \'Add Token. 
>
> -   Name it something descriptive and select \'Add\'.
>
>     ![](images/Screen%20Shot%202023-08-24%20at%204.18.46%20pm.png)
>
> -   Copy the token to your clipboard.
>
>     ![](images/Screen%20Shot%202023-08-24%20at%204.18.58%20pm.png)
>
> -   Log in to NCI Gadi HPC on the command-line and navigate to \$HOME.
>
> -   Make a hidden directory called tower to store Tower token and credentials in. 
>
> -   Copy your personal access token into a file named token within this directory.
>
>     ![](images/Screen%20Shot%202023-08-24%20at%204.19.09%20pm.png)

### Create your personal Tower credential for NCI Gadi

You can create different types of credentials through the Tower interface, for accessing code repositories, commercial cloud, SSH-keys, and the Tower Agent. To create a personal credential you will need to navigate to the user settings logo in the top navbar and select \'Your credentials\'. 

![](images/Screen%20Shot%202023-08-24%20at%204.21.36%20pm.png)

### Create a Tower credential for NCI Gadi

You can either choose to work with a shared or personal credential. Setting this up requires a similar process to creating a personal access token, using both the CLI and Tower interface. Starting on the Tower interface, navigate to \'Credentials\' in the top navbar menu:

> -   Select \'Add Credential\'.
>
> -   Name the credential something descriptive.
>
> -   Select Tower Agent from the Provider dropdown menu.
>
> -   Enable \'Shared agent\' so that all workspace users can access the same instance (optional).
>
> -   Copy the Agent connection ID to your clipboard.
>
>     ![](images/Screen%20Shot%202023-08-24%20at%204.21.47%20pm.png)
>
> -   Before saving the credential you will need to run the agent on your HPC command line. 
>
> -   Log in to your HPC project space on the command-line and navigate to the shared space where you\'ll be running the tower agent from. 
>
> -   Copy your personal access token into a file named connection_id within the hidden .tower directory.
>
>     ![](images/Screen%20Shot%202023-08-24%20at%204.25.20%20pm.png)
>
> ```{=html}
> <!-- -->
> ```
> -   Run the run_tower_agent_pbs.sh script at /g/data/er01/tower-nf.
>
>     ![](images/Screen%20Shot%202023-08-24%20at%204.25.31%20pm.png)
>
> -   Back in the tower interface, hit 'Add' to save your Tower agent credential.
>
> -   You can cancel the Tower agent running on the command line after the credential is saved.

### Configure a compute environment 

Compute environments have been configured for our group on NCI Gadi and Pawsey Setonix HPCs. Currently only NCI Gadi is a shared environment. The NCI Gadi environment has specifically been configured for our SIH project \'er01\'. Only users with access to er01 added to our workspace will be able to run workflows using this configured compute environment. The process for setting up this workspace is as follows: 

> -   Run the tower agent on the HPC command lineCLI again with the [relevant run_tower_agent\_\<pbs\|slurm\>.sh script](https://github.com/Sydney-Informatics-Hub/tower-nf).
>
> -   In the Tower interface, navigate to \'Compute Environments\' in the top navbar menu.
>
> -   Name the environment something descriptive to distinguish it from other environments you may set up.
>
> -   Select the platform. For Setonix this is Slurm workload manager, for Gadi this is Altair PBS Pro. 
>
> -   Provide your (shared or personal) credentials previously saved to the connection_id file.
>
>     ![](images/Screen%20Shot%202023-08-24%20at%204.29.29%20pm.png)

For those working on Gadi:

> -   It\'s important that you leave the head and compute queue names empty, given jobs submitted to queues on Gadi are transferred to the exec version of each queue once running. The exec queue names are not visible to the Tower API so your jobs will fail to run. 
>
> -   Under \'Staging options\' Singularity and Nextflow modules have been preloaded to be executed before the pipeline launches 
>
> -   Under \'Advanced options\' we have set the Nextflow queue size as 1000 (as directed [here](https://opus.nci.org.au/display/Help/Queue+Limits))
>
> -   Under \'Advanced options\' we have set the Head job submit #PBS options and applied head jobs to the compute jobs :
>
>     -   Walltime 10 hours 
>
>     -   NCPUS 1 
>
>     -   Memory 8G 
>
>     -   Access to /scratch/iz89
>
>     -   Project iz89 
>
>     -   Queue: copyq (as this provides external network access)
>
>         ![](images/Screen%20Shot%202023-08-24%20at%204.30.19%20pm.png)

For those working on Setonix:

> -   TBA
>
>     -   TBA

\
\
